{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_lstm_packed_imdb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq8GOpX6wZjm",
        "colab_type": "text"
      },
      "source": [
        "Deep Learning Models -- A collection of various deep learning architectures, models, and tips for TensorFlow and PyTorch in Jupyter Notebooks.\n",
        "- Author: Sebastian Raschka\n",
        "- GitHub Repository: https://github.com/rasbt/deeplearning-models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vY4SK0xKAJgm"
      },
      "source": [
        "# Bidirectional Multi-layer RNN with LSTM with Own Dataset in CSV Format (AG News)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC_Wk6EFwZjo",
        "colab_type": "text"
      },
      "source": [
        "Dataset Description\n",
        "\n",
        "```\n",
        "AG's News Topic Classification Dataset\n",
        "\n",
        "Version 3, Updated 09/09/2015\n",
        "\n",
        "\n",
        "ORIGIN\n",
        "\n",
        "AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000  news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic community for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity. For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\n",
        "\n",
        "The AG's news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the dataset above. It is used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n",
        "\n",
        "\n",
        "DESCRIPTION\n",
        "\n",
        "The AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.\n",
        "\n",
        "The file classes.txt contains a list of classes corresponding to each label.\n",
        "\n",
        "The files train.csv and test.csv contain all the training samples as comma-sparated values. There are 3 columns in them, corresponding to class index (1 to 4), title and description. The title and description are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). New lines are escaped by a backslash followed with an \"n\" character, that is \"\\n\".\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fzbgh0lwd03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q IPython\n",
        "!pip install -q ipykernel\n",
        "!pip install -q watermark\n",
        "!pip install -q matplotlib\n",
        "!pip install -q sklearn\n",
        "!pip install -q pandas\n",
        "!pip install -q pydot\n",
        "!pip install -q hiddenlayer\n",
        "!pip install -q graphviz\n",
        "!pip install -q gdown"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "moNmVfuvnImW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "3772c607-64d0-4abb-b254-763ab54adbe3"
      },
      "source": [
        "%load_ext watermark\n",
        "%watermark -a 'Sebastian Raschka' -v -p torch\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sebastian Raschka \n",
            "\n",
            "CPython 3.6.9\n",
            "IPython 5.5.0\n",
            "\n",
            "torch 1.5.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GSRL42Qgy8I8"
      },
      "source": [
        "## General Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OvW1RgfepCBq",
        "colab": {}
      },
      "source": [
        "RANDOM_SEED = 123\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "VOCABULARY_SIZE = 5000\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 50\n",
        "DROPOUT = 0.5\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "BIDIRECTIONAL = True\n",
        "HIDDEN_DIM = 256\n",
        "NUM_LAYERS = 2\n",
        "OUTPUT_DIM = 4"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mQMmKUEisW4W"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxdcJy57wZj1",
        "colab_type": "text"
      },
      "source": [
        "The AG News dataset is available from Xiang Zhang's Google Drive folder at\n",
        "\n",
        "https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M\n",
        "\n",
        "From the Google Drive folder, download the file \n",
        "\n",
        "- `ag_news_csv.tar.gz`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXsRy6cgwqq0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "dd0286e8-7f71-4a9a-99ed-99ad054e24dc"
      },
      "source": [
        "!gdown -O ag_news_csv.tar.gz --id 0Bz8a_Dbh9QhbUDNpeUdjb0wxRms"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=0Bz8a_Dbh9QhbUDNpeUdjb0wxRms\n",
            "To: /content/ag_news_csv.tar.gz\n",
            "11.8MB [00:00, 54.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAXVMdoHwZj1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "236f881c-0adc-49b9-c3a3-12d1bfe5175d"
      },
      "source": [
        "!tar xvzf  ag_news_csv.tar.gz"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ag_news_csv/\n",
            "ag_news_csv/train.csv\n",
            "ag_news_csv/test.csv\n",
            "ag_news_csv/classes.txt\n",
            "ag_news_csv/readme.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfX5GQt1wZj3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "e3216d1d-9a97-4fe0-ffdc-7720fc01195f"
      },
      "source": [
        "!cat ag_news_csv/classes.txt"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "World\n",
            "Sports\n",
            "Business\n",
            "Sci/Tech\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-YuJgRYwZj6",
        "colab_type": "text"
      },
      "source": [
        "Check that the dataset looks okay:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "762zKRc8wZj6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "4eb5d432-d258-4b82-9de4-0e52c1781d71"
      },
      "source": [
        "df = pd.read_csv('ag_news_csv/train.csv', header=None, index_col=None)\n",
        "df.columns = ['classlabel', 'title', 'content']\n",
        "df['classlabel'] = df['classlabel']-1\n",
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classlabel</th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
              "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
              "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
              "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
              "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>Oil prices soar to all-time record, posing new...</td>\n",
              "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   classlabel  ...                                            content\n",
              "0           2  ...  Reuters - Short-sellers, Wall Street's dwindli...\n",
              "1           2  ...  Reuters - Private investment firm Carlyle Grou...\n",
              "2           2  ...  Reuters - Soaring crude prices plus worries\\ab...\n",
              "3           2  ...  Reuters - Authorities have halted oil export\\f...\n",
              "4           2  ...  AFP - Tearaway world oil prices, toppling reco...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kqp4vFLvwZj9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "85efd1f2-956a-46ef-80cf-b1180e287088"
      },
      "source": [
        "np.unique(df['classlabel'].values)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHzpre8-wZj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "be61eab5-d6cc-4dcc-a078-e5f26c2e544a"
      },
      "source": [
        "np.bincount(df['classlabel'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([30000, 30000, 30000, 30000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNlelyUBwZkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[['classlabel', 'content']].to_csv('ag_news_csv/train_prepocessed.csv', index=None)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aRhOBqPwZkE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "dedd0240-b798-4b6e-b721-f396ed181557"
      },
      "source": [
        "df = pd.read_csv('ag_news_csv/test.csv', header=None, index_col=None)\n",
        "df.columns = ['classlabel', 'title', 'content']\n",
        "df['classlabel'] = df['classlabel']-1\n",
        "df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classlabel</th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Fears for T N pension after talks</td>\n",
              "      <td>Unions representing workers at Turner   Newall...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>The Race is On: Second Private Team Sets Launc...</td>\n",
              "      <td>SPACE.com - TORONTO, Canada -- A second\\team o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Ky. Company Wins Grant to Study Peptides (AP)</td>\n",
              "      <td>AP - A company founded by a chemistry research...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Prediction Unit Helps Forecast Wildfires (AP)</td>\n",
              "      <td>AP - It's barely dawn when Mike Fitzpatrick st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>Calif. Aims to Limit Farm-Related Smog (AP)</td>\n",
              "      <td>AP - Southern California's smog-fighting agenc...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   classlabel  ...                                            content\n",
              "0           2  ...  Unions representing workers at Turner   Newall...\n",
              "1           3  ...  SPACE.com - TORONTO, Canada -- A second\\team o...\n",
              "2           3  ...  AP - A company founded by a chemistry research...\n",
              "3           3  ...  AP - It's barely dawn when Mike Fitzpatrick st...\n",
              "4           3  ...  AP - Southern California's smog-fighting agenc...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7OqWf_EwZkG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "55c71400-72fb-4768-81c9-243a21a083df"
      },
      "source": [
        "np.unique(df['classlabel'].values)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXq8zuG3wZkI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e70411d6-50e5-4009-d1b2-6a6ad41f2402"
      },
      "source": [
        "np.bincount(df['classlabel'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1900, 1900, 1900, 1900])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc-gyzhIwZkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[['classlabel', 'content']].to_csv('ag_news_csv/test_prepocessed.csv', index=None)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1c3x8TPwZkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del df"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4GnH64XvsV8n"
      },
      "source": [
        "Define the Label and Text field formatters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GrUene-wZkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(sequential=True,\n",
        "                  tokenize='spacy',\n",
        "                  include_lengths=True) # necessary for packed_padded_sequence\n",
        "\n",
        "LABEL = data.LabelField(dtype=torch.float)\n",
        "\n",
        "\n",
        "# If you get an error [E050] Can't find model 'en'\n",
        "# you need to run the following on your command line:\n",
        "#  python -m spacy download en"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3azGDS4iwZkR",
        "colab_type": "text"
      },
      "source": [
        "Process the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nkr1QirwZkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fields = [('classlabel', LABEL), ('content', TEXT)]\n",
        "\n",
        "train_dataset = data.TabularDataset(\n",
        "    path=\"ag_news_csv/train_prepocessed.csv\", format='csv',\n",
        "    skip_header=True, fields=fields)\n",
        "\n",
        "test_dataset = data.TabularDataset(\n",
        "    path=\"ag_news_csv/test_prepocessed.csv\", format='csv',\n",
        "    skip_header=True, fields=fields)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C002-NnwZkU",
        "colab_type": "text"
      },
      "source": [
        "Split the training dataset into training and validation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WZ_4jiHVnMxN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d03526a5-2c5d-4ac2-e651-e4855cd615ee"
      },
      "source": [
        "train_data, valid_data = train_dataset.split(\n",
        "    split_ratio=[0.95, 0.05],\n",
        "    random_state=random.seed(RANDOM_SEED))\n",
        "\n",
        "print(f'Num Train: {len(train_data)}')\n",
        "print(f'Num Valid: {len(valid_data)}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num Train: 114000\n",
            "Num Valid: 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L-TBwKWPslPa"
      },
      "source": [
        "Build the vocabulary based on the top \"VOCABULARY_SIZE\" words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e8uNrjdtn4A8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "94117aa0-bbac-4ee0-a1dc-9a40dbe53208"
      },
      "source": [
        "TEXT.build_vocab(train_data,\n",
        "                 max_size=VOCABULARY_SIZE,\n",
        "                 vectors='glove.6B.100d',\n",
        "                 unk_init=torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
        "print(f'Number of classes: {len(LABEL.vocab)}')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:29, 2.22MB/s]                          \n",
            "100%|█████████▉| 398407/400000 [00:15<00:00, 25295.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 5002\n",
            "Number of classes: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZVp7PPBwZkZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "428abc42-549a-41c0-9409-faf7def09ba6"
      },
      "source": [
        "list(LABEL.vocab.freqs)[-10:]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1', '3', '0', '2']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JpEMNInXtZsb"
      },
      "source": [
        "The TEXT.vocab dictionary will contain the word counts and indices. The reason why the number of words is VOCABULARY_SIZE + 2 is that it contains to special tokens for padding and unknown words: `<unk>` and `<pad>`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eIQ_zfKLwjKm"
      },
      "source": [
        "Make dataset iterators:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i7JiHR1stHNF",
        "colab": {}
      },
      "source": [
        "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_dataset), \n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_within_batch=True, # necessary for packed_padded_sequence\n",
        "    sort_key=lambda x: len(x.content),\n",
        "    device=DEVICE)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R0pT_dMRvicQ"
      },
      "source": [
        "Testing the iterators (note that the number of rows depends on the longest document in the respective batch):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y8SP_FccutT0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "5998019e-97e4-4891-df70-c001c1f749bb"
      },
      "source": [
        "print('Train')\n",
        "for batch in train_loader:\n",
        "    print(f'Text matrix size: {batch.content[0].size()}')\n",
        "    print(f'Target vector size: {batch.classlabel.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nValid:')\n",
        "for batch in valid_loader:\n",
        "    print(f'Text matrix size: {batch.content[0].size()}')\n",
        "    print(f'Target vector size: {batch.classlabel.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nTest:')\n",
        "for batch in test_loader:\n",
        "    print(f'Text matrix size: {batch.content[0].size()}')\n",
        "    print(f'Target vector size: {batch.classlabel.size()}')\n",
        "    break"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "Text matrix size: torch.Size([35, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Valid:\n",
            "Text matrix size: torch.Size([17, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Test:\n",
            "Text matrix size: torch.Size([16, 128])\n",
            "Target vector size: torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G_grdW3pxCzz"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nQIUm5EjxFNa",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, bidirectional, hidden_dim, num_layers, output_dim, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim,\n",
        "                           num_layers=num_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        self.fc1 = nn.Linear(hidden_dim * num_layers, 64)\n",
        "        self.fc2 = nn.Linear(64, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_length):\n",
        "\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_length)\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        # output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        hidden = self.fc1(hidden)\n",
        "        hidden = self.dropout(hidden)\n",
        "        hidden = self.fc2(hidden)\n",
        "        return hidden"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ik3NF3faxFmZ",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, BIDIRECTIONAL, HIDDEN_DIM, NUM_LAYERS, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
        "model = model.to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjbeb1hJw-UY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        },
        "outputId": "e74f83f6-5ba9-4880-f78a-8981e2f4486a"
      },
      "source": [
        "import hiddenlayer as hl\n",
        "batch = next(iter(train_loader))\n",
        "hl.build_graph(model, batch.content)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/onnx/symbolic_opset9.py:1310: UserWarning: Dropout is a training op and should not be exported in inference mode. Make sure to call eval() on the model, and to export it with param training=False.\n",
            "  warnings.warn(\"Dropout is a training op and should not be exported in inference mode. \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/onnx/symbolic_opset9.py:1586: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
            "  \"or define the initial states (h0/c0) as inputs of the model. \")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/onnx/symbolic_helper.py:176: UserWarning: ONNX export failed on RNN/GRU/LSTM because dropout in training mode not supported\n",
            "  warnings.warn(\"ONNX export failed on \" + op + \" because \" + msg + \" not supported\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<hiddenlayer.graph.Graph at 0x7f0a6acc9f28>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"1511pt\" height=\"521pt\"\n viewBox=\"0.00 0.00 1511.00 521.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(72 485)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-72,36 -72,-485 1439,-485 1439,36 -72,36\"/>\n<!-- /outputs/23 -->\n<g id=\"node1\" class=\"node\">\n<title>/outputs/23</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"54,-449 0,-449 0,-413 54,-413 54,-449\"/>\n<text text-anchor=\"start\" x=\"13\" y=\"-428\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Gather</text>\n</g>\n<!-- /outputs/24/25 -->\n<g id=\"node2\" class=\"node\">\n<title>/outputs/24/25</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"197,-449 143,-449 143,-413 197,-413 197,-449\"/>\n<text text-anchor=\"start\" x=\"153\" y=\"-428\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Dropout</text>\n</g>\n<!-- /outputs/23&#45;&gt;/outputs/24/25 -->\n<g id=\"edge1\" class=\"edge\">\n<title>/outputs/23&#45;&gt;/outputs/24/25</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M54.2338,-431C76.4983,-431 108.2039,-431 132.8066,-431\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"132.8676,-434.5001 142.8676,-431 132.8676,-427.5001 132.8676,-434.5001\"/>\n<text text-anchor=\"middle\" x=\"98.5\" y=\"-434\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">28x128x128</text>\n</g>\n<!-- /outputs/28/29 -->\n<g id=\"node3\" class=\"node\">\n<title>/outputs/28/29</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"377,-414 286,-414 286,-378 377,-378 377,-414\"/>\n<text text-anchor=\"start\" x=\"294.5\" y=\"-393\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">prim::PackPadded</text>\n</g>\n<!-- /outputs/24/25&#45;&gt;/outputs/28/29 -->\n<g id=\"edge2\" class=\"edge\">\n<title>/outputs/24/25&#45;&gt;/outputs/28/29</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M197.0315,-425.1418C218.4501,-420.5 249.0968,-413.8583 275.7246,-408.0875\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"276.6739,-411.4632 285.7057,-405.9245 275.1912,-404.622 276.6739,-411.4632\"/>\n<text text-anchor=\"middle\" x=\"241.5\" y=\"-423\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">28x128x128</text>\n</g>\n<!-- /outputs/37/38/39 -->\n<g id=\"node11\" class=\"node\">\n<title>/outputs/37/38/39</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"513,-225 456,-225 456,-189 513,-189 513,-225\"/>\n<text text-anchor=\"start\" x=\"464.5\" y=\"-204\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">aten::lstm</text>\n</g>\n<!-- /outputs/28/29&#45;&gt;/outputs/37/38/39 -->\n<g id=\"edge3\" class=\"edge\">\n<title>/outputs/28/29&#45;&gt;/outputs/37/38/39</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M365.2407,-377.8328C369.3585,-375.0862 373.3833,-372.1198 377,-369 421.5052,-330.6095 455.2023,-269.4938 472.2036,-234.3297\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"475.4384,-235.6764 476.5512,-225.1402 469.1108,-232.6828 475.4384,-235.6764\"/>\n<text text-anchor=\"middle\" x=\"416.5\" y=\"-353\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">3531x128</text>\n</g>\n<!-- /outputs/30 -->\n<g id=\"node4\" class=\"node\">\n<title>/outputs/30</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"358.5,-360 304.5,-360 304.5,-324 358.5,-324 358.5,-360\"/>\n<text text-anchor=\"start\" x=\"313.5\" y=\"-339\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Constant</text>\n</g>\n<!-- /outputs/30&#45;&gt;/outputs/37/38/39 -->\n<g id=\"edge4\" class=\"edge\">\n<title>/outputs/30&#45;&gt;/outputs/37/38/39</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M358.6432,-327.2986C364.8574,-323.5532 371.3057,-319.3482 377,-315 409.3507,-290.2972 441.6458,-256.2764 462.2666,-233.0611\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"465.0847,-235.1563 469.0544,-225.3332 459.8254,-230.5367 465.0847,-235.1563\"/>\n</g>\n<!-- /outputs/31 -->\n<g id=\"node5\" class=\"node\">\n<title>/outputs/31</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"358.5,-306 304.5,-306 304.5,-270 358.5,-270 358.5,-306\"/>\n<text text-anchor=\"start\" x=\"313.5\" y=\"-285\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Constant</text>\n</g>\n<!-- /outputs/31&#45;&gt;/outputs/37/38/39 -->\n<g id=\"edge5\" class=\"edge\">\n<title>/outputs/31&#45;&gt;/outputs/37/38/39</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M358.6757,-273.7983C380.218,-262.5194 411.0709,-246.3186 438,-232 440.9134,-230.4509 443.927,-228.8431 446.9549,-227.2237\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"448.8355,-230.1869 455.9961,-222.3781 445.5288,-224.0171 448.8355,-230.1869\"/>\n</g>\n<!-- /outputs/32 -->\n<g id=\"node6\" class=\"node\">\n<title>/outputs/32</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"358.5,-252 304.5,-252 304.5,-216 358.5,-216 358.5,-252\"/>\n<text text-anchor=\"start\" x=\"313.5\" y=\"-231\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Constant</text>\n</g>\n<!-- /outputs/32&#45;&gt;/outputs/37/38/39 -->\n<g id=\"edge6\" class=\"edge\">\n<title>/outputs/32&#45;&gt;/outputs/37/38/39</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M358.5011,-229.2351C382.7673,-224.9528 418.6783,-218.6156 445.9609,-213.801\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"446.7129,-217.2225 455.9525,-212.0378 445.4963,-210.329 446.7129,-217.2225\"/>\n</g>\n<!-- /outputs/33 -->\n<g id=\"node7\" class=\"node\">\n<title>/outputs/33</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"358.5,-198 304.5,-198 304.5,-162 358.5,-162 358.5,-198\"/>\n<text text-anchor=\"start\" x=\"313.5\" y=\"-177\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Constant</text>\n</g>\n<!-- /outputs/33&#45;&gt;/outputs/37/38/39 -->\n<g id=\"edge7\" class=\"edge\">\n<title>/outputs/33&#45;&gt;/outputs/37/38/39</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M358.5011,-184.7649C382.7673,-189.0472 418.6783,-195.3844 445.9609,-200.199\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"445.4963,-203.671 455.9525,-201.9622 446.7129,-196.7775 445.4963,-203.671\"/>\n</g>\n<!-- /outputs/34 -->\n<g id=\"node8\" class=\"node\">\n<title>/outputs/34</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"358.5,-144 304.5,-144 304.5,-108 358.5,-108 358.5,-144\"/>\n<text text-anchor=\"start\" x=\"313.5\" y=\"-123\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Constant</text>\n</g>\n<!-- /outputs/34&#45;&gt;/outputs/37/38/39 -->\n<g id=\"edge8\" class=\"edge\">\n<title>/outputs/34&#45;&gt;/outputs/37/38/39</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M358.5011,-140.2947C383.088,-153.3113 419.6296,-172.6568 447.038,-187.1672\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"445.4769,-190.3009 455.9525,-191.8866 448.7522,-184.1144 445.4769,-190.3009\"/>\n</g>\n<!-- /outputs/35 -->\n<g id=\"node9\" class=\"node\">\n<title>/outputs/35</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"358.5,-90 304.5,-90 304.5,-54 358.5,-54 358.5,-90\"/>\n<text text-anchor=\"start\" x=\"313.5\" y=\"-69\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Constant</text>\n</g>\n<!-- /outputs/35&#45;&gt;/outputs/37/38/39 -->\n<g id=\"edge9\" class=\"edge\">\n<title>/outputs/35&#45;&gt;/outputs/37/38/39</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M358.6163,-86.7367C364.8316,-90.4807 371.2872,-94.6762 377,-99 409.4559,-123.5645 441.7298,-157.6132 462.3168,-180.8728\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"459.8734,-183.3955 469.0924,-188.6168 465.1416,-178.7861 459.8734,-183.3955\"/>\n</g>\n<!-- /outputs/36 -->\n<g id=\"node10\" class=\"node\">\n<title>/outputs/36</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"358.5,-36 304.5,-36 304.5,0 358.5,0 358.5,-36\"/>\n<text text-anchor=\"start\" x=\"313.5\" y=\"-15\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Constant</text>\n</g>\n<!-- /outputs/36&#45;&gt;/outputs/37/38/39 -->\n<g id=\"edge10\" class=\"edge\">\n<title>/outputs/36&#45;&gt;/outputs/37/38/39</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M358.7982,-32.5034C365.006,-36.257 371.4126,-40.5153 377,-45 407.8967,-69.7987 415.6189,-77.3095 438,-110 452.982,-131.8831 465.651,-159.3364 473.963,-179.4464\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"470.8082,-180.9817 477.793,-188.948 477.3006,-178.3646 470.8082,-180.9817\"/>\n</g>\n<!-- /outputs/40 -->\n<g id=\"node12\" class=\"node\">\n<title>/outputs/40</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"651,-252 597,-252 597,-216 651,-216 651,-252\"/>\n<text text-anchor=\"start\" x=\"614\" y=\"-231\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Slice</text>\n</g>\n<!-- /outputs/37/38/39&#45;&gt;/outputs/40 -->\n<g id=\"edge11\" class=\"edge\">\n<title>/outputs/37/38/39&#45;&gt;/outputs/40</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M513.0665,-212.529C534.453,-216.6683 563.8874,-222.3653 587.0626,-226.8508\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"586.4107,-230.2895 596.8936,-228.7536 587.7409,-223.4171 586.4107,-230.2895\"/>\n<text text-anchor=\"middle\" x=\"555\" y=\"-228\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">4x128x256</text>\n</g>\n<!-- /outputs/42 -->\n<g id=\"node14\" class=\"node\">\n<title>/outputs/42</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"651,-198 597,-198 597,-162 651,-162 651,-198\"/>\n<text text-anchor=\"start\" x=\"614\" y=\"-177\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Slice</text>\n</g>\n<!-- /outputs/37/38/39&#45;&gt;/outputs/42 -->\n<g id=\"edge12\" class=\"edge\">\n<title>/outputs/37/38/39&#45;&gt;/outputs/42</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M513.0665,-201.471C534.453,-197.3317 563.8874,-191.6347 587.0626,-187.1492\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"587.7409,-190.5829 596.8936,-185.2464 586.4107,-183.7105 587.7409,-190.5829\"/>\n<text text-anchor=\"middle\" x=\"555\" y=\"-200\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">4x128x256</text>\n</g>\n<!-- /outputs/41 -->\n<g id=\"node13\" class=\"node\">\n<title>/outputs/41</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"742,-252 688,-252 688,-216 742,-216 742,-252\"/>\n<text text-anchor=\"start\" x=\"698\" y=\"-231\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Squeeze</text>\n</g>\n<!-- /outputs/40&#45;&gt;/outputs/41 -->\n<g id=\"edge13\" class=\"edge\">\n<title>/outputs/40&#45;&gt;/outputs/41</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M651.303,-234C659.5813,-234 668.8217,-234 677.6405,-234\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"677.7163,-237.5001 687.7163,-234 677.7162,-230.5001 677.7163,-237.5001\"/>\n</g>\n<!-- /outputs/44 -->\n<g id=\"node16\" class=\"node\">\n<title>/outputs/44</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"869,-225 815,-225 815,-189 869,-189 869,-225\"/>\n<text text-anchor=\"start\" x=\"827\" y=\"-204\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Concat</text>\n</g>\n<!-- /outputs/41&#45;&gt;/outputs/44 -->\n<g id=\"edge14\" class=\"edge\">\n<title>/outputs/41&#45;&gt;/outputs/44</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M742.2447,-228.2078C760.492,-224.3285 784.746,-219.1721 804.7693,-214.9152\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"805.7468,-218.2857 814.8004,-212.7826 804.2911,-211.4387 805.7468,-218.2857\"/>\n<text text-anchor=\"middle\" x=\"778.5\" y=\"-226\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">128x256</text>\n</g>\n<!-- /outputs/43 -->\n<g id=\"node15\" class=\"node\">\n<title>/outputs/43</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"742,-198 688,-198 688,-162 742,-162 742,-198\"/>\n<text text-anchor=\"start\" x=\"698\" y=\"-177\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Squeeze</text>\n</g>\n<!-- /outputs/42&#45;&gt;/outputs/43 -->\n<g id=\"edge15\" class=\"edge\">\n<title>/outputs/42&#45;&gt;/outputs/43</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M651.303,-180C659.5813,-180 668.8217,-180 677.6405,-180\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"677.7163,-183.5001 687.7163,-180 677.7162,-176.5001 677.7163,-183.5001\"/>\n</g>\n<!-- /outputs/43&#45;&gt;/outputs/44 -->\n<g id=\"edge16\" class=\"edge\">\n<title>/outputs/43&#45;&gt;/outputs/44</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M742.2447,-185.7922C760.492,-189.6715 784.746,-194.8279 804.7693,-199.0848\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"804.2911,-202.5613 814.8004,-201.2174 805.7468,-195.7143 804.2911,-202.5613\"/>\n<text text-anchor=\"middle\" x=\"778.5\" y=\"-199\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">128x256</text>\n</g>\n<!-- /outputs/45/46 -->\n<g id=\"node17\" class=\"node\">\n<title>/outputs/45/46</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"996,-225 942,-225 942,-189 996,-189 996,-225\"/>\n<text text-anchor=\"start\" x=\"952\" y=\"-204\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Dropout</text>\n</g>\n<!-- /outputs/44&#45;&gt;/outputs/45/46 -->\n<g id=\"edge17\" class=\"edge\">\n<title>/outputs/44&#45;&gt;/outputs/45/46</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M869.2447,-207C887.492,-207 911.746,-207 931.7693,-207\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"931.8004,-210.5001 941.8004,-207 931.8003,-203.5001 931.8004,-210.5001\"/>\n<text text-anchor=\"middle\" x=\"905.5\" y=\"-210\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">128x512</text>\n</g>\n<!-- /outputs/47 -->\n<g id=\"node18\" class=\"node\">\n<title>/outputs/47</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1123,-225 1069,-225 1069,-189 1123,-189 1123,-225\"/>\n<text text-anchor=\"start\" x=\"1083\" y=\"-204\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Linear</text>\n</g>\n<!-- /outputs/45/46&#45;&gt;/outputs/47 -->\n<g id=\"edge18\" class=\"edge\">\n<title>/outputs/45/46&#45;&gt;/outputs/47</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M996.2447,-207C1014.492,-207 1038.746,-207 1058.7693,-207\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1058.8004,-210.5001 1068.8004,-207 1058.8003,-203.5001 1058.8004,-210.5001\"/>\n<text text-anchor=\"middle\" x=\"1032.5\" y=\"-210\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">128x512</text>\n</g>\n<!-- /outputs/48/49 -->\n<g id=\"node19\" class=\"node\">\n<title>/outputs/48/49</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1245,-225 1191,-225 1191,-189 1245,-189 1245,-225\"/>\n<text text-anchor=\"start\" x=\"1201\" y=\"-204\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Dropout</text>\n</g>\n<!-- /outputs/47&#45;&gt;/outputs/48/49 -->\n<g id=\"edge19\" class=\"edge\">\n<title>/outputs/47&#45;&gt;/outputs/48/49</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1123.0758,-207C1140.0553,-207 1162.1767,-207 1180.7924,-207\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1181,-210.5001 1190.9999,-207 1180.9999,-203.5001 1181,-210.5001\"/>\n<text text-anchor=\"middle\" x=\"1157\" y=\"-210\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">128x64</text>\n</g>\n<!-- /outputs/50 -->\n<g id=\"node20\" class=\"node\">\n<title>/outputs/50</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1367,-225 1313,-225 1313,-189 1367,-189 1367,-225\"/>\n<text text-anchor=\"start\" x=\"1327\" y=\"-204\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Linear</text>\n</g>\n<!-- /outputs/48/49&#45;&gt;/outputs/50 -->\n<g id=\"edge20\" class=\"edge\">\n<title>/outputs/48/49&#45;&gt;/outputs/50</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1245.0758,-207C1262.0553,-207 1284.1767,-207 1302.7924,-207\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1303,-210.5001 1312.9999,-207 1302.9999,-203.5001 1303,-210.5001\"/>\n<text text-anchor=\"middle\" x=\"1279\" y=\"-210\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">128x64</text>\n</g>\n<!-- 4937772249435845478 -->\n<g id=\"node21\" class=\"node\">\n<title>4937772249435845478</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"197,-395 143,-395 143,-351 197,-351 197,-395\"/>\n<text text-anchor=\"start\" x=\"161\" y=\"-379\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Cast</text>\n<text text-anchor=\"start\" x=\"173\" y=\"-358\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">x2</text>\n</g>\n<!-- 4937772249435845478&#45;&gt;/outputs/28/29 -->\n<g id=\"edge21\" class=\"edge\">\n<title>4937772249435845478&#45;&gt;/outputs/28/29</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M197.0315,-376.8497C218.4501,-379.9 249.0968,-384.2646 275.7246,-388.0568\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"275.312,-391.5332 285.7057,-389.4782 276.299,-384.6032 275.312,-391.5332\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lv9Ny9di6VcI"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T5t1Afn4xO11",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_data in enumerate(data_loader):\n",
        "            text, text_lengths = batch_data.content\n",
        "            logits = model(text, text_lengths)\n",
        "            _, predicted_labels = torch.max(logits, 1)\n",
        "            num_examples += batch_data.classlabel.size(0)\n",
        "            correct_pred += (predicted_labels.long() == batch_data.classlabel.long()).sum()\n",
        "        return correct_pred.float()/num_examples * 100"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EABZM8Vo0ilB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "218c6d28-ddb9-49e0-c883-4f61292fd39e"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    for batch_idx, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        text, text_lengths = batch_data.content\n",
        "        \n",
        "        ### FORWARD AND BACK PROP\n",
        "        logits = model(text, text_lengths)\n",
        "        cost = F.cross_entropy(logits, batch_data.classlabel.long())\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        cost.backward()\n",
        "        \n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        ### LOGGING\n",
        "        if not batch_idx % 50:\n",
        "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
        "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
        "                   f'Cost: {cost:.4f}')\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        print(f'training accuracy: '\n",
        "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
        "              f'\\nvalid accuracy: '\n",
        "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
        "        \n",
        "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
        "    \n",
        "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
        "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001/050 | Batch 000/891 | Cost: 1.3865\n",
            "Epoch: 001/050 | Batch 050/891 | Cost: 1.2066\n",
            "Epoch: 001/050 | Batch 100/891 | Cost: 1.1692\n",
            "Epoch: 001/050 | Batch 150/891 | Cost: 0.9862\n",
            "Epoch: 001/050 | Batch 200/891 | Cost: 0.9992\n",
            "Epoch: 001/050 | Batch 250/891 | Cost: 0.8206\n",
            "Epoch: 001/050 | Batch 300/891 | Cost: 0.7449\n",
            "Epoch: 001/050 | Batch 350/891 | Cost: 0.6300\n",
            "Epoch: 001/050 | Batch 400/891 | Cost: 0.7949\n",
            "Epoch: 001/050 | Batch 450/891 | Cost: 0.5924\n",
            "Epoch: 001/050 | Batch 500/891 | Cost: 0.6359\n",
            "Epoch: 001/050 | Batch 550/891 | Cost: 0.9071\n",
            "Epoch: 001/050 | Batch 600/891 | Cost: 0.5533\n",
            "Epoch: 001/050 | Batch 650/891 | Cost: 0.6460\n",
            "Epoch: 001/050 | Batch 700/891 | Cost: 0.5671\n",
            "Epoch: 001/050 | Batch 750/891 | Cost: 0.4732\n",
            "Epoch: 001/050 | Batch 800/891 | Cost: 0.4709\n",
            "Epoch: 001/050 | Batch 850/891 | Cost: 0.4538\n",
            "training accuracy: 86.64%\n",
            "valid accuracy: 85.98%\n",
            "Time elapsed: 44.74 min\n",
            "Epoch: 002/050 | Batch 000/891 | Cost: 0.5731\n",
            "Epoch: 002/050 | Batch 050/891 | Cost: 0.4066\n",
            "Epoch: 002/050 | Batch 100/891 | Cost: 0.5296\n",
            "Epoch: 002/050 | Batch 150/891 | Cost: 0.4164\n",
            "Epoch: 002/050 | Batch 200/891 | Cost: 0.7092\n",
            "Epoch: 002/050 | Batch 250/891 | Cost: 0.2924\n",
            "Epoch: 002/050 | Batch 300/891 | Cost: 0.4751\n",
            "Epoch: 002/050 | Batch 350/891 | Cost: 0.4885\n",
            "Epoch: 002/050 | Batch 400/891 | Cost: 0.4146\n",
            "Epoch: 002/050 | Batch 450/891 | Cost: 0.4086\n",
            "Epoch: 002/050 | Batch 500/891 | Cost: 0.4126\n",
            "Epoch: 002/050 | Batch 550/891 | Cost: 0.4081\n",
            "Epoch: 002/050 | Batch 600/891 | Cost: 0.3983\n",
            "Epoch: 002/050 | Batch 650/891 | Cost: 0.3676\n",
            "Epoch: 002/050 | Batch 700/891 | Cost: 0.3549\n",
            "Epoch: 002/050 | Batch 750/891 | Cost: 0.4006\n",
            "Epoch: 002/050 | Batch 800/891 | Cost: 0.3311\n",
            "Epoch: 002/050 | Batch 850/891 | Cost: 0.3934\n",
            "training accuracy: 89.49%\n",
            "valid accuracy: 88.58%\n",
            "Time elapsed: 89.11 min\n",
            "Epoch: 003/050 | Batch 000/891 | Cost: 0.3795\n",
            "Epoch: 003/050 | Batch 050/891 | Cost: 0.3105\n",
            "Epoch: 003/050 | Batch 100/891 | Cost: 0.3409\n",
            "Epoch: 003/050 | Batch 150/891 | Cost: 0.2980\n",
            "Epoch: 003/050 | Batch 200/891 | Cost: 0.3524\n",
            "Epoch: 003/050 | Batch 250/891 | Cost: 0.3270\n",
            "Epoch: 003/050 | Batch 300/891 | Cost: 0.2899\n",
            "Epoch: 003/050 | Batch 350/891 | Cost: 0.6125\n",
            "Epoch: 003/050 | Batch 400/891 | Cost: 0.4674\n",
            "Epoch: 003/050 | Batch 450/891 | Cost: 0.4709\n",
            "Epoch: 003/050 | Batch 500/891 | Cost: 0.3356\n",
            "Epoch: 003/050 | Batch 550/891 | Cost: 0.4814\n",
            "Epoch: 003/050 | Batch 600/891 | Cost: 0.4929\n",
            "Epoch: 003/050 | Batch 650/891 | Cost: 0.3862\n",
            "Epoch: 003/050 | Batch 700/891 | Cost: 0.3506\n",
            "Epoch: 003/050 | Batch 750/891 | Cost: 0.2761\n",
            "Epoch: 003/050 | Batch 800/891 | Cost: 0.2897\n",
            "Epoch: 003/050 | Batch 850/891 | Cost: 0.2195\n",
            "training accuracy: 90.45%\n",
            "valid accuracy: 89.65%\n",
            "Time elapsed: 132.96 min\n",
            "Epoch: 004/050 | Batch 000/891 | Cost: 0.4080\n",
            "Epoch: 004/050 | Batch 050/891 | Cost: 0.4662\n",
            "Epoch: 004/050 | Batch 100/891 | Cost: 0.2239\n",
            "Epoch: 004/050 | Batch 150/891 | Cost: 0.2426\n",
            "Epoch: 004/050 | Batch 200/891 | Cost: 0.3059\n",
            "Epoch: 004/050 | Batch 250/891 | Cost: 0.3569\n",
            "Epoch: 004/050 | Batch 300/891 | Cost: 0.2312\n",
            "Epoch: 004/050 | Batch 350/891 | Cost: 0.2386\n",
            "Epoch: 004/050 | Batch 400/891 | Cost: 0.2386\n",
            "Epoch: 004/050 | Batch 450/891 | Cost: 0.4447\n",
            "Epoch: 004/050 | Batch 500/891 | Cost: 0.3575\n",
            "Epoch: 004/050 | Batch 550/891 | Cost: 0.2721\n",
            "Epoch: 004/050 | Batch 600/891 | Cost: 0.3490\n",
            "Epoch: 004/050 | Batch 650/891 | Cost: 0.7210\n",
            "Epoch: 004/050 | Batch 700/891 | Cost: 0.4410\n",
            "Epoch: 004/050 | Batch 750/891 | Cost: 0.3475\n",
            "Epoch: 004/050 | Batch 800/891 | Cost: 0.4778\n",
            "Epoch: 004/050 | Batch 850/891 | Cost: 0.2533\n",
            "training accuracy: 91.03%\n",
            "valid accuracy: 90.17%\n",
            "Time elapsed: 176.71 min\n",
            "Epoch: 005/050 | Batch 000/891 | Cost: 0.5619\n",
            "Epoch: 005/050 | Batch 050/891 | Cost: 0.3035\n",
            "Epoch: 005/050 | Batch 100/891 | Cost: 0.3130\n",
            "Epoch: 005/050 | Batch 150/891 | Cost: 0.5711\n",
            "Epoch: 005/050 | Batch 200/891 | Cost: 0.3044\n",
            "Epoch: 005/050 | Batch 250/891 | Cost: 0.3087\n",
            "Epoch: 005/050 | Batch 300/891 | Cost: 0.4370\n",
            "Epoch: 005/050 | Batch 350/891 | Cost: 0.2578\n",
            "Epoch: 005/050 | Batch 400/891 | Cost: 0.3212\n",
            "Epoch: 005/050 | Batch 450/891 | Cost: 0.2605\n",
            "Epoch: 005/050 | Batch 500/891 | Cost: 0.3084\n",
            "Epoch: 005/050 | Batch 550/891 | Cost: 0.3047\n",
            "Epoch: 005/050 | Batch 600/891 | Cost: 0.2465\n",
            "Epoch: 005/050 | Batch 650/891 | Cost: 0.2496\n",
            "Epoch: 005/050 | Batch 700/891 | Cost: 0.2339\n",
            "Epoch: 005/050 | Batch 750/891 | Cost: 0.2255\n",
            "Epoch: 005/050 | Batch 800/891 | Cost: 0.3510\n",
            "Epoch: 005/050 | Batch 850/891 | Cost: 0.3325\n",
            "training accuracy: 91.82%\n",
            "valid accuracy: 90.45%\n",
            "Time elapsed: 220.54 min\n",
            "Epoch: 006/050 | Batch 000/891 | Cost: 0.2420\n",
            "Epoch: 006/050 | Batch 050/891 | Cost: 0.3390\n",
            "Epoch: 006/050 | Batch 100/891 | Cost: 0.1702\n",
            "Epoch: 006/050 | Batch 150/891 | Cost: 0.1867\n",
            "Epoch: 006/050 | Batch 200/891 | Cost: 0.3012\n",
            "Epoch: 006/050 | Batch 250/891 | Cost: 0.3091\n",
            "Epoch: 006/050 | Batch 300/891 | Cost: 0.2583\n",
            "Epoch: 006/050 | Batch 350/891 | Cost: 0.3542\n",
            "Epoch: 006/050 | Batch 400/891 | Cost: 0.2488\n",
            "Epoch: 006/050 | Batch 450/891 | Cost: 0.2597\n",
            "Epoch: 006/050 | Batch 500/891 | Cost: 0.2361\n",
            "Epoch: 006/050 | Batch 550/891 | Cost: 0.2397\n",
            "Epoch: 006/050 | Batch 600/891 | Cost: 0.2089\n",
            "Epoch: 006/050 | Batch 650/891 | Cost: 0.3217\n",
            "Epoch: 006/050 | Batch 700/891 | Cost: 0.2800\n",
            "Epoch: 006/050 | Batch 750/891 | Cost: 0.2834\n",
            "Epoch: 006/050 | Batch 800/891 | Cost: 0.4064\n",
            "Epoch: 006/050 | Batch 850/891 | Cost: 0.2371\n",
            "training accuracy: 92.22%\n",
            "valid accuracy: 90.48%\n",
            "Time elapsed: 264.21 min\n",
            "Epoch: 007/050 | Batch 000/891 | Cost: 0.2109\n",
            "Epoch: 007/050 | Batch 050/891 | Cost: 0.2562\n",
            "Epoch: 007/050 | Batch 100/891 | Cost: 0.2449\n",
            "Epoch: 007/050 | Batch 150/891 | Cost: 0.3115\n",
            "Epoch: 007/050 | Batch 200/891 | Cost: 0.2027\n",
            "Epoch: 007/050 | Batch 250/891 | Cost: 0.2840\n",
            "Epoch: 007/050 | Batch 300/891 | Cost: 0.2568\n",
            "Epoch: 007/050 | Batch 350/891 | Cost: 0.2564\n",
            "Epoch: 007/050 | Batch 400/891 | Cost: 0.3099\n",
            "Epoch: 007/050 | Batch 450/891 | Cost: 0.2767\n",
            "Epoch: 007/050 | Batch 500/891 | Cost: 0.3347\n",
            "Epoch: 007/050 | Batch 550/891 | Cost: 0.4921\n",
            "Epoch: 007/050 | Batch 600/891 | Cost: 0.2172\n",
            "Epoch: 007/050 | Batch 650/891 | Cost: 0.2893\n",
            "Epoch: 007/050 | Batch 700/891 | Cost: 0.2324\n",
            "Epoch: 007/050 | Batch 750/891 | Cost: 0.2343\n",
            "Epoch: 007/050 | Batch 800/891 | Cost: 0.3307\n",
            "Epoch: 007/050 | Batch 850/891 | Cost: 0.1284\n",
            "training accuracy: 92.81%\n",
            "valid accuracy: 91.02%\n",
            "Time elapsed: 307.98 min\n",
            "Epoch: 008/050 | Batch 000/891 | Cost: 0.3653\n",
            "Epoch: 008/050 | Batch 050/891 | Cost: 0.2144\n",
            "Epoch: 008/050 | Batch 100/891 | Cost: 0.2268\n",
            "Epoch: 008/050 | Batch 150/891 | Cost: 0.1437\n",
            "Epoch: 008/050 | Batch 200/891 | Cost: 0.1932\n",
            "Epoch: 008/050 | Batch 250/891 | Cost: 0.3421\n",
            "Epoch: 008/050 | Batch 300/891 | Cost: 0.3322\n",
            "Epoch: 008/050 | Batch 350/891 | Cost: 0.5034\n",
            "Epoch: 008/050 | Batch 400/891 | Cost: 0.1997\n",
            "Epoch: 008/050 | Batch 450/891 | Cost: 0.3117\n",
            "Epoch: 008/050 | Batch 500/891 | Cost: 0.2752\n",
            "Epoch: 008/050 | Batch 550/891 | Cost: 0.2060\n",
            "Epoch: 008/050 | Batch 600/891 | Cost: 0.2657\n",
            "Epoch: 008/050 | Batch 650/891 | Cost: 0.2148\n",
            "Epoch: 008/050 | Batch 700/891 | Cost: 0.5220\n",
            "Epoch: 008/050 | Batch 750/891 | Cost: 0.2100\n",
            "Epoch: 008/050 | Batch 800/891 | Cost: 0.2695\n",
            "Epoch: 008/050 | Batch 850/891 | Cost: 0.1166\n",
            "training accuracy: 93.15%\n",
            "valid accuracy: 91.20%\n",
            "Time elapsed: 351.89 min\n",
            "Epoch: 009/050 | Batch 000/891 | Cost: 0.1679\n",
            "Epoch: 009/050 | Batch 050/891 | Cost: 0.1663\n",
            "Epoch: 009/050 | Batch 100/891 | Cost: 0.1377\n",
            "Epoch: 009/050 | Batch 150/891 | Cost: 0.2380\n",
            "Epoch: 009/050 | Batch 200/891 | Cost: 0.4676\n",
            "Epoch: 009/050 | Batch 250/891 | Cost: 0.1650\n",
            "Epoch: 009/050 | Batch 300/891 | Cost: 0.2062\n",
            "Epoch: 009/050 | Batch 350/891 | Cost: 0.2386\n",
            "Epoch: 009/050 | Batch 400/891 | Cost: 0.1854\n",
            "Epoch: 009/050 | Batch 450/891 | Cost: 0.1619\n",
            "Epoch: 009/050 | Batch 500/891 | Cost: 0.2721\n",
            "Epoch: 009/050 | Batch 550/891 | Cost: 0.2177\n",
            "Epoch: 009/050 | Batch 600/891 | Cost: 0.2399\n",
            "Epoch: 009/050 | Batch 650/891 | Cost: 0.3334\n",
            "Epoch: 009/050 | Batch 700/891 | Cost: 0.1921\n",
            "Epoch: 009/050 | Batch 750/891 | Cost: 0.2253\n",
            "Epoch: 009/050 | Batch 800/891 | Cost: 0.2322\n",
            "Epoch: 009/050 | Batch 850/891 | Cost: 0.2424\n",
            "training accuracy: 93.68%\n",
            "valid accuracy: 91.17%\n",
            "Time elapsed: 396.10 min\n",
            "Epoch: 010/050 | Batch 000/891 | Cost: 0.1630\n",
            "Epoch: 010/050 | Batch 050/891 | Cost: 0.6210\n",
            "Epoch: 010/050 | Batch 100/891 | Cost: 0.1098\n",
            "Epoch: 010/050 | Batch 150/891 | Cost: 0.3321\n",
            "Epoch: 010/050 | Batch 200/891 | Cost: 0.1632\n",
            "Epoch: 010/050 | Batch 250/891 | Cost: 0.3805\n",
            "Epoch: 010/050 | Batch 300/891 | Cost: 0.1848\n",
            "Epoch: 010/050 | Batch 350/891 | Cost: 0.2241\n",
            "Epoch: 010/050 | Batch 400/891 | Cost: 0.1455\n",
            "Epoch: 010/050 | Batch 450/891 | Cost: 0.1809\n",
            "Epoch: 010/050 | Batch 500/891 | Cost: 0.1230\n",
            "Epoch: 010/050 | Batch 550/891 | Cost: 0.1528\n",
            "Epoch: 010/050 | Batch 600/891 | Cost: 0.2650\n",
            "Epoch: 010/050 | Batch 650/891 | Cost: 0.5526\n",
            "Epoch: 010/050 | Batch 700/891 | Cost: 0.2332\n",
            "Epoch: 010/050 | Batch 750/891 | Cost: 0.2867\n",
            "Epoch: 010/050 | Batch 800/891 | Cost: 0.3292\n",
            "Epoch: 010/050 | Batch 850/891 | Cost: 0.1677\n",
            "training accuracy: 93.96%\n",
            "valid accuracy: 91.38%\n",
            "Time elapsed: 440.32 min\n",
            "Epoch: 011/050 | Batch 000/891 | Cost: 0.2626\n",
            "Epoch: 011/050 | Batch 050/891 | Cost: 0.3735\n",
            "Epoch: 011/050 | Batch 100/891 | Cost: 0.3422\n",
            "Epoch: 011/050 | Batch 150/891 | Cost: 0.3251\n",
            "Epoch: 011/050 | Batch 200/891 | Cost: 0.2137\n",
            "Epoch: 011/050 | Batch 250/891 | Cost: 0.4341\n",
            "Epoch: 011/050 | Batch 300/891 | Cost: 0.2544\n",
            "Epoch: 011/050 | Batch 350/891 | Cost: 0.2753\n",
            "Epoch: 011/050 | Batch 400/891 | Cost: 0.2500\n",
            "Epoch: 011/050 | Batch 450/891 | Cost: 0.1807\n",
            "Epoch: 011/050 | Batch 500/891 | Cost: 0.2634\n",
            "Epoch: 011/050 | Batch 550/891 | Cost: 0.1809\n",
            "Epoch: 011/050 | Batch 600/891 | Cost: 0.2623\n",
            "Epoch: 011/050 | Batch 650/891 | Cost: 0.1828\n",
            "Epoch: 011/050 | Batch 700/891 | Cost: 0.1820\n",
            "Epoch: 011/050 | Batch 750/891 | Cost: 0.1596\n",
            "Epoch: 011/050 | Batch 800/891 | Cost: 0.2049\n",
            "Epoch: 011/050 | Batch 850/891 | Cost: 0.2318\n",
            "training accuracy: 94.19%\n",
            "valid accuracy: 91.37%\n",
            "Time elapsed: 484.63 min\n",
            "Epoch: 012/050 | Batch 000/891 | Cost: 0.2473\n",
            "Epoch: 012/050 | Batch 050/891 | Cost: 0.2711\n",
            "Epoch: 012/050 | Batch 100/891 | Cost: 0.2087\n",
            "Epoch: 012/050 | Batch 150/891 | Cost: 0.2078\n",
            "Epoch: 012/050 | Batch 200/891 | Cost: 0.1564\n",
            "Epoch: 012/050 | Batch 250/891 | Cost: 0.4019\n",
            "Epoch: 012/050 | Batch 300/891 | Cost: 0.2119\n",
            "Epoch: 012/050 | Batch 350/891 | Cost: 0.1586\n",
            "Epoch: 012/050 | Batch 400/891 | Cost: 0.1586\n",
            "Epoch: 012/050 | Batch 450/891 | Cost: 0.1959\n",
            "Epoch: 012/050 | Batch 500/891 | Cost: 0.2860\n",
            "Epoch: 012/050 | Batch 550/891 | Cost: 0.1592\n",
            "Epoch: 012/050 | Batch 600/891 | Cost: 0.2029\n",
            "Epoch: 012/050 | Batch 650/891 | Cost: 0.1411\n",
            "Epoch: 012/050 | Batch 700/891 | Cost: 0.1636\n",
            "Epoch: 012/050 | Batch 750/891 | Cost: 0.4585\n",
            "Epoch: 012/050 | Batch 800/891 | Cost: 0.3303\n",
            "Epoch: 012/050 | Batch 850/891 | Cost: 0.2053\n",
            "training accuracy: 94.25%\n",
            "valid accuracy: 91.27%\n",
            "Time elapsed: 529.07 min\n",
            "Epoch: 013/050 | Batch 000/891 | Cost: 0.1727\n",
            "Epoch: 013/050 | Batch 050/891 | Cost: 0.2319\n",
            "Epoch: 013/050 | Batch 100/891 | Cost: 0.2547\n",
            "Epoch: 013/050 | Batch 150/891 | Cost: 0.1629\n",
            "Epoch: 013/050 | Batch 200/891 | Cost: 0.2687\n",
            "Epoch: 013/050 | Batch 250/891 | Cost: 0.1243\n",
            "Epoch: 013/050 | Batch 300/891 | Cost: 0.1653\n",
            "Epoch: 013/050 | Batch 350/891 | Cost: 0.2273\n",
            "Epoch: 013/050 | Batch 400/891 | Cost: 0.1439\n",
            "Epoch: 013/050 | Batch 450/891 | Cost: 0.1079\n",
            "Epoch: 013/050 | Batch 500/891 | Cost: 0.1597\n",
            "Epoch: 013/050 | Batch 550/891 | Cost: 0.1183\n",
            "Epoch: 013/050 | Batch 600/891 | Cost: 0.1983\n",
            "Epoch: 013/050 | Batch 650/891 | Cost: 0.1694\n",
            "Epoch: 013/050 | Batch 700/891 | Cost: 0.1173\n",
            "Epoch: 013/050 | Batch 750/891 | Cost: 0.4300\n",
            "Epoch: 013/050 | Batch 800/891 | Cost: 0.1649\n",
            "Epoch: 013/050 | Batch 850/891 | Cost: 0.2319\n",
            "training accuracy: 94.48%\n",
            "valid accuracy: 91.33%\n",
            "Time elapsed: 573.85 min\n",
            "Epoch: 014/050 | Batch 000/891 | Cost: 0.3350\n",
            "Epoch: 014/050 | Batch 050/891 | Cost: 0.1493\n",
            "Epoch: 014/050 | Batch 100/891 | Cost: 0.1915\n",
            "Epoch: 014/050 | Batch 150/891 | Cost: 0.1542\n",
            "Epoch: 014/050 | Batch 200/891 | Cost: 0.2878\n",
            "Epoch: 014/050 | Batch 250/891 | Cost: 0.3289\n",
            "Epoch: 014/050 | Batch 300/891 | Cost: 0.2222\n",
            "Epoch: 014/050 | Batch 350/891 | Cost: 0.1825\n",
            "Epoch: 014/050 | Batch 400/891 | Cost: 0.1595\n",
            "Epoch: 014/050 | Batch 450/891 | Cost: 0.1994\n",
            "Epoch: 014/050 | Batch 500/891 | Cost: 0.2591\n",
            "Epoch: 014/050 | Batch 550/891 | Cost: 0.0813\n",
            "Epoch: 014/050 | Batch 600/891 | Cost: 0.2934\n",
            "Epoch: 014/050 | Batch 650/891 | Cost: 0.2297\n",
            "Epoch: 014/050 | Batch 700/891 | Cost: 0.1085\n",
            "Epoch: 014/050 | Batch 750/891 | Cost: 0.4476\n",
            "Epoch: 014/050 | Batch 800/891 | Cost: 0.2017\n",
            "Epoch: 014/050 | Batch 850/891 | Cost: 0.1037\n",
            "training accuracy: 94.92%\n",
            "valid accuracy: 91.83%\n",
            "Time elapsed: 620.51 min\n",
            "Epoch: 015/050 | Batch 000/891 | Cost: 0.3512\n",
            "Epoch: 015/050 | Batch 050/891 | Cost: 0.2270\n",
            "Epoch: 015/050 | Batch 100/891 | Cost: 0.1123\n",
            "Epoch: 015/050 | Batch 150/891 | Cost: 0.1457\n",
            "Epoch: 015/050 | Batch 200/891 | Cost: 0.2095\n",
            "Epoch: 015/050 | Batch 250/891 | Cost: 0.2736\n",
            "Epoch: 015/050 | Batch 300/891 | Cost: 0.1769\n",
            "Epoch: 015/050 | Batch 350/891 | Cost: 0.1727\n",
            "Epoch: 015/050 | Batch 400/891 | Cost: 0.1513\n",
            "Epoch: 015/050 | Batch 450/891 | Cost: 0.2565\n",
            "Epoch: 015/050 | Batch 500/891 | Cost: 0.0815\n",
            "Epoch: 015/050 | Batch 550/891 | Cost: 0.1166\n",
            "Epoch: 015/050 | Batch 600/891 | Cost: 0.2503\n",
            "Epoch: 015/050 | Batch 650/891 | Cost: 0.3061\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3071-LUJwZkp",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FarxSLblwZkp",
        "colab_type": "text"
      },
      "source": [
        "Evaluating on some new text that has been collected from recent news articles and is not part of the training or test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jt55pscgFdKZ",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "\n",
        "map_dictionary = {\n",
        "    0: \"World\",\n",
        "    1: \"Sports\",\n",
        "    2: \"Business\",\n",
        "    3:\"Sci/Tech\",\n",
        "}\n",
        "\n",
        "\n",
        "def predict_class(model, sentence, min_len=4):\n",
        "    # Somewhat based on\n",
        "    # https://github.com/bentrevett/pytorch-sentiment-analysis/\n",
        "    # blob/master/5%20-%20Multi-class%20Sentiment%20Analysis.ipynb\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    if len(tokenized) < min_len:\n",
        "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(DEVICE)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    preds = model(tensor, length_tensor)\n",
        "    preds = torch.softmax(preds, dim=1)\n",
        "    \n",
        "    proba, class_label = preds.max(dim=1)\n",
        "    return proba.item(), class_label.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b-vpnB7wZks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"\"\"\n",
        "The windfall follows a tender offer by Z Holdings, which is controlled by SoftBank’s domestic wireless unit, \n",
        "for half of Zozo’s shares this month.\n",
        "\"\"\"\n",
        "\n",
        "proba, pred_label = predict_class(model, text)\n",
        "\n",
        "print(f'Class Label: {pred_label} -> {map_dictionary[pred_label]}')\n",
        "print(f'Probability: {proba}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhgsYdMlwZku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"\"\"\n",
        "EU data regulator issues first-ever sanction of an EU institution, \n",
        "against the European parliament over its use of US-based NationBuilder to process voter data \n",
        "\"\"\"\n",
        "\n",
        "proba, pred_label = predict_class(model, text)\n",
        "\n",
        "print(f'Class Label: {pred_label} -> {map_dictionary[pred_label]}')\n",
        "print(f'Probability: {proba}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk1tFKnnwZkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"\"\"\n",
        "LG announces CEO Jo Seong-jin will be replaced by Brian Kwon Dec. 1, amid 2020 \n",
        "leadership shakeup and LG smartphone division's 18th straight quarterly loss\n",
        "\"\"\"\n",
        "\n",
        "proba, pred_label = predict_class(model, text)\n",
        "\n",
        "print(f'Class Label: {pred_label} -> {map_dictionary[pred_label]}')\n",
        "print(f'Probability: {proba}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7lRusB3dF80X",
        "colab": {}
      },
      "source": [
        "%watermark -iv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQJjNaZiwZk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}