# TF-IDF

TF-IDF stands for “Term Frequency — Inverse Data Frequency”. First, we will learn what this term means mathematically.

## TF

Term Frequency (tf) gives us the frequency of the word in each document in the corpus. It is the ratio of number of times the word appears in a document compared to the total number of words in that document. It increases as the number of occurrences of that word within the document increases. Each document has its own tf.

Basically, it is possible to say that TF is identical to the BoW (Bag of Words).

## IDF

Inverse Data Frequency (idf): used to calculate the weight of rare words across all documents in the corpus. The words that occur rarely in the corpus have a high IDF score.

idf(w, d) = N / df

N = the number of total documents, and df = the number of documents that conatain the word w. You could find that the equation above might have error if the value of df is 0, since we cannot devide some number by 0. Thus, we need to do the smoothing.

idf(w, d) = N / (df + 1)

Now, the equation will not face the "devide by 0" error, since the value of the df should be greater than or equal to 0. Now, the range of the value of idf is (1, N]. To control the range of the result vector, now we will could apply the logarithm as below.

idf(w, d) = log(N / (df + 1))

We know that log(1) is 0, so I would replace the value N to N + 1, so that the minimum value of idf(w,d) could be log(N+1/N+1) = log(1) = 0.

idf (w, d) = log((N + 1) / (df + 1))

## Multiplying TF and IDF

tf-idf(w, d) = tf(w, d) * idf(w, d)

Since, tf is identical to BoW, it is possible to say that:

tf-idf(w, d) = BoW(w, d) * idf(w, d)

## Difference between TF-IDF, word2vec, and GloVe

Glove and Word2vec are both unsupervised models for generating word vectors. The difference between them is the mechanism of generating word vectors.

The word vectors generated by either of these models can be used for a wide variety of tasks ranging such as:

    1) finding words that are semantically similar to a word

    2) representing a word when it is being input to a downstream model. 
    
        A word embedding representation of a word captures more information about a word than just a one-hot representation of the word, since the former captures semantic similarity of that word to other words whereas the latter representation of the word is equidistant from all other words.

Tf-idf is a scoring scheme for words - that is a measure of how important a word is to a document.

From a practical usage standpoint, while tf-idf is a simple scoring scheme and that is its key advantage, word embeddings may be a better choice for most tasks where tf-idf is used, particularly when the task can benefit from the semantic similarity captured by word embeddings (e.g. in information retrieval tasks).

## L2 Normalisation

When calculating the tf-idf vector, the tf-idf method performs the feature scaling method to all columns in the document matrix. Clearly, this column scaling method is same thing with the L2 normalisation. Therefore, it is possible to say that the tf-idf does the L2 normalisation internally.
