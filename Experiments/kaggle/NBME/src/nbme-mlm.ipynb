{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"ref: https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/discussion/323095","metadata":{}},{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# This must be done before importing transformers\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    \n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:18:12.909594Z","iopub.execute_input":"2022-05-07T01:18:12.909814Z","iopub.status.idle":"2022-05-07T01:18:12.941347Z","shell.execute_reply.started":"2022-05-07T01:18:12.909788Z","shell.execute_reply":"2022-05-07T01:18:12.940658Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%writefile mlm.py\n\nimport argparse\nimport os\nimport json\nfrom pathlib import Path\n\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport torch\nfrom datasets import load_dataset\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoConfig\nfrom transformers import DataCollatorForLanguageModeling, AutoModelForMaskedLM, Trainer\nfrom transformers import TrainingArguments\nfrom transformers.utils import logging\nfrom IPython import embed  # noqa\n\nlogging.set_verbosity_info()\nlogger = logging.get_logger(__name__)\nlogger.info(\"INFO\")\nlogger.warning(\"WARN\")\nKAGGLE_ENV = True if 'KAGGLE_URL_BASE' in set(os.environ.keys()) else False\n\n\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nINPUT_DIR = Path('../input/')\nif KAGGLE_ENV:\n    OUTPUT_DIR = Path('')\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\nelse:\n    OUTPUT_DIR = INPUT_DIR\n\n\ndef get_patient_notes_not_used_train():\n\n    patient_notes = pd.read_csv(INPUT_DIR / 'nbme-score-clinical-patient-notes' / \"patient_notes.csv\")\n    print(patient_notes.shape)\n    train = pd.read_csv(INPUT_DIR / 'nbme-score-clinical-patient-notes' / 'train.csv')\n    train_pn_num_unique = train['pn_num'].unique()\n\n    train_patient_notes = \\\n        patient_notes.loc[~patient_notes['pn_num'].isin(train_pn_num_unique), :].reset_index(drop=True)\n    valid_patient_notes = \\\n        patient_notes.loc[patient_notes['pn_num'].isin(train_pn_num_unique), :].reset_index(drop=True)\n\n    print(train_patient_notes.shape)\n    print(valid_patient_notes.shape)\n    return train_patient_notes, valid_patient_notes\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"])\n\n\ndef get_tokenizer(args):\n    if 'v3' in str(args.model_path):\n        from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n        print('DebertaV2TokenizerFast')\n        tokenizer = DebertaV2TokenizerFast.from_pretrained(INPUT_DIR / args.model_path, trim_offsets=False)\n    else:\n        if args.model_name:\n            print('model_name', args.model_name)\n            tokenizer = AutoTokenizer.from_pretrained(args.model_name, trim_offsets=False)\n        else:\n            print('model_path', args.model_path)\n            tokenizer = AutoTokenizer.from_pretrained(INPUT_DIR / args.model_path, trim_offsets=False)\n    return tokenizer\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_name\", type=str, default=\"\", required=False)\n    parser.add_argument(\"--model_path\", type=str, default=\"../input/deberta-v3-large/deberta-v3-large/\", required=False)\n    parser.add_argument(\"--seed\", type=int, default=0, required=False)\n    parser.add_argument('--debug', action='store_true', required=False)\n    parser.add_argument('--exp_num', type=str, required=True)\n    parser.add_argument(\"--param_freeze\", action='store_true', required=False)\n    parser.add_argument(\"--num_train_epochs\", type=int, default=5, required=False)\n    parser.add_argument(\"--batch_size\", type=int, default=8, required=False)\n    parser.add_argument(\"--lr\", type=float, default=2e-5, required=False)\n    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1, required=False)\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n\n    args = parse_args()\n    train, valid = get_patient_notes_not_used_train()\n\n    if args.debug:\n        train = train.iloc[:10, :]\n        valid = valid.iloc[:10, :]\n        args.batch_seize = 1\n\n    def get_text(df):\n        text_list = []\n        for text in tqdm(df['pn_history']):\n            if len(text) < 30:\n                pass\n            else:\n                text_list.append(text)\n        return text_list\n\n    train_text_list = get_text(train)\n    valid_text_list = get_text(valid)\n\n    mlm_train_json_path = OUTPUT_DIR / 'train_mlm.json'\n    mlm_valid_json_path = OUTPUT_DIR / 'valid_mlm.json'\n\n    for json_path, list_ in zip([mlm_train_json_path, mlm_valid_json_path],\n                                [train_text_list, valid_text_list]):\n        with open(str(json_path), 'w') as f:\n            for sentence in list_:\n                row_json = {'text': sentence}\n                json.dump(row_json, f)\n                f.write('\\n')\n\n    datasets = load_dataset(\n        'json',\n        data_files={'train': str(mlm_train_json_path),\n                    'valid': str(mlm_valid_json_path)},\n        )\n\n    if mlm_train_json_path.is_file():\n        mlm_train_json_path.unlink()\n    if mlm_valid_json_path.is_file():\n        mlm_valid_json_path.unlink()\n    print(datasets[\"train\"][:2])\n\n    tokenizer = get_tokenizer(args)\n\n    tokenized_datasets = datasets.map(\n        tokenize_function,\n        batched=True,\n        num_proc=1,\n        remove_columns=[\"text\"],\n        batch_size=args.batch_size)\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n\n    if args.model_name:\n        print('model_name:', args.model_name)\n        model_name = args.model_name\n    else:\n        print('model_path:', args.model_path)\n        model_name = INPUT_DIR / args.model_path\n    config = AutoConfig.from_pretrained(model_name, output_hidden_states=True)\n\n    if 'v3' in str(model_name):\n        model = transformers.DebertaV2ForMaskedLM.from_pretrained(INPUT_DIR / model_name, config=config)\n    else:\n        model = AutoModelForMaskedLM.from_pretrained(model_name, config=config)\n\n    if args.param_freeze:\n        # if freeze, Write freeze settings here\n\n        # deberta-v3-large\n        # model.deberta.embeddings.requires_grad_(False)\n        # model.deberta.encoder.layer[:12].requires_grad_(False)\n\n        # deberta-large\n        model.deberta.embeddings.requires_grad_(False)\n        model.deberta.encoder.layer[:24].requires_grad_(False)\n\n        for name, p in model.named_parameters():\n            print(name, p.requires_grad)\n\n    if args.debug:\n        save_steps = 100\n        args.num_train_epochs = 1\n    else:\n        save_steps = 100000000\n\n    training_args = TrainingArguments(\n        output_dir=\"output-mlm\",\n        evaluation_strategy=\"epoch\",\n        learning_rate=args.lr,\n        weight_decay=0.01,\n        save_strategy='no',\n        per_device_train_batch_size=args.batch_size,\n        num_train_epochs=args.num_train_epochs,\n        # report_to=\"wandb\",\n        run_name=f'output-mlm-{args.exp_num}',\n        # logging_dir='./logs',\n        lr_scheduler_type='cosine',\n        warmup_ratio=0.2,\n        fp16=True,\n        logging_steps=500,\n        gradient_accumulation_steps=args.gradient_accumulation_steps\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_datasets[\"train\"],\n        eval_dataset=tokenized_datasets['valid'],\n        data_collator=data_collator,\n        # optimizers=(optimizer, scheduler)\n    )\n\n    trainer.train()\n\n    if args.model_name == 'microsoft/deberta-xlarge':\n        model_name = 'deberta-xlarge'\n    elif args.model_name == 'microsoft/deberta-large':\n        model_name = 'deberta-large'\n    elif args.model_name == 'microsoft/deberta-base':\n        model_name = 'deberta-base'\n    elif args.model_path == \"../input/deberta-v3-large/deberta-v3-large/\":\n        model_name = 'deberta-v3-large'\n    elif args.model_name == 'microsoft/deberta-v2-xlarge':\n        model_name = 'deberta-v2-xlarge'\n    trainer.model.save_pretrained(OUTPUT_DIR / f'{args.exp_num}_mlm_{model_name}')\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-07T01:18:12.943273Z","iopub.execute_input":"2022-05-07T01:18:12.943677Z","iopub.status.idle":"2022-05-07T01:18:12.955692Z","shell.execute_reply.started":"2022-05-07T01:18:12.943642Z","shell.execute_reply":"2022-05-07T01:18:12.954855Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Writing mlm.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python mlm.py --debug --exp_num 0","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:18:12.958488Z","iopub.execute_input":"2022-05-07T01:18:12.959200Z","iopub.status.idle":"2022-05-07T01:18:52.479549Z","shell.execute_reply.started":"2022-05-07T01:18:12.959172Z","shell.execute_reply":"2022-05-07T01:18:52.477939Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"tokenizers.__version__: 0.11.6\ntransformers.__version__: 4.16.2\n(42146, 3)\n(41146, 3)\n(1000, 3)\n100%|████████████████████████████████████████| 10/10 [00:00<00:00, 91379.17it/s]\n100%|███████████████████████████████████████| 10/10 [00:00<00:00, 120525.98it/s]\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-666cc1054dba211c/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 9788.34it/s]\n100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1608.25it/s]\nDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-666cc1054dba211c/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 955.75it/s]\n{'text': [\"17-year-old male, has come to the student health clinic complaining of heart pounding. Mr. Cleveland's mother has given verbal consent for a history, physical examination, and treatment\\r\\n-began 2-3 months ago,sudden,intermittent for 2 days(lasting 3-4 min),worsening,non-allev/aggrav\\r\\n-associated with dispnea on exersion and rest,stressed out about school\\r\\n-reports fe feels like his heart is jumping out of his chest\\r\\n-ros:denies chest pain,dyaphoresis,wt loss,chills,fever,nausea,vomiting,pedal edeam\\r\\n-pmh:non,meds :aderol (from a friend),nkda\\r\\n-fh:father had MI recently,mother has thyroid dz\\r\\n-sh:non-smoker,mariguana 5-6 months ago,3 beers on the weekend, basketball at school\\r\\n-sh:no std\", '17 yo male with recurrent palpitations for the past 3 mo lasting about 3 - 4 min, it happened about 5 - 6 times since the beginning. One time durign a baskeball game two days ago light headedness, pressure in the chest, catching breath, but no fainting. During teh episodes no sweating. No diarrhea, no heat intolerance, no weight loss. Has tried aterol to be able to better concentrate, has received it from his roommate. .']}\nDebertaV2TokenizerFast\nDidn't find file ../input/../input/deberta-v3-large/deberta-v3-large/added_tokens.json. We won't load it.\nDidn't find file ../input/../input/deberta-v3-large/deberta-v3-large/special_tokens_map.json. We won't load it.\nDidn't find file ../input/../input/deberta-v3-large/deberta-v3-large/tokenizer.json. We won't load it.\nloading file ../input/../input/deberta-v3-large/deberta-v3-large/spm.model\nloading file None\nloading file None\nloading file ../input/../input/deberta-v3-large/deberta-v3-large/tokenizer_config.json\nloading file None\nloading configuration file ../input/../input/deberta-v3-large/deberta-v3-large/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"../input/../input/deberta-v3-large/deberta-v3-large\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nAdding [MASK] to the vocabulary\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file ../input/../input/deberta-v3-large/deberta-v3-large/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"../input/../input/deberta-v3-large/deberta-v3-large\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 66.27ba/s]\n100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 200.01ba/s]\nmodel_path: ../input/deberta-v3-large/deberta-v3-large/\nloading configuration file ../input/../input/deberta-v3-large/deberta-v3-large/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"../input/../input/deberta-v3-large/deberta-v3-large\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.16.2\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file ../input/../input/../input/deberta-v3-large/deberta-v3-large/pytorch_model.bin\nSome weights of the model checkpoint at ../input/../input/../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifer.weight', 'mask_predictions.classifer.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'deberta.embeddings.position_embeddings.weight']\n- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at ../input/../input/../input/deberta-v3-large/deberta-v3-large and are newly initialized: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing amp half precision backend\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 10\n  Num Epochs = 1\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2\n  0%|                                                     | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n  args.max_grad_norm,\n100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.02s/it]***** Running Evaluation *****\n  Num examples = 10\n  Batch size = 8\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 12.90161418914795, 'eval_runtime': 0.4835, 'eval_samples_per_second': 20.682, 'eval_steps_per_second': 4.136, 'epoch': 1.0}\n100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.02s/it]\n100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 11.76it/s]\u001b[A\n                                                                                \u001b[A\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 2.8258, 'train_samples_per_second': 3.539, 'train_steps_per_second': 0.708, 'train_loss': 12.558328628540039, 'epoch': 1.0}\n100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.40s/it]\nConfiguration saved in 0_mlm_deberta-v3-large/config.json\nModel weights saved in 0_mlm_deberta-v3-large/pytorch_model.bin\n","output_type":"stream"}]},{"cell_type":"code","source":"ls ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:18:52.483853Z","iopub.execute_input":"2022-05-07T01:18:52.484131Z","iopub.status.idle":"2022-05-07T01:18:53.191078Z","shell.execute_reply.started":"2022-05-07T01:18:52.484100Z","shell.execute_reply":"2022-05-07T01:18:53.190263Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\u001b[0m\u001b[01;34m0_mlm_deberta-v3-large\u001b[0m/  __notebook_source__.ipynb  mlm.py  \u001b[01;34moutput-mlm\u001b[0m/\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}